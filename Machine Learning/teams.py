# -*- coding: utf-8 -*-
"""Teams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBeL4xJt84Ogm6xEHUuQhmwfQHmdmPiW
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df= pd.read_csv('teams.csv')
print(df)

df= df[["team","country","year", "athletes","age", "prev_medals","medals"]]
df=df.dropna()

X=df.drop(['team','country','year','medals'], axis=1,)
y=df['medals']

X

X=X.values
y=y.values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=41)

# Prepare inputs properly
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

print(X_train.shape)  # should be [N, 3]

import torch
import torch.nn as nn
import torch.nn.functional as f

class Model(nn.Module):
  def __init__(self, inp_features=3, h1=8, h2=9, out_features=1):
    super().__init__()
    self.fc1= nn.Linear(inp_features,h1)
    self.fc2= nn.Linear(h1,h2)
    self.out= nn.Linear(h2,out_features)

  def forward(self,x):
    x=f.relu(self.fc1(x))
    x=f.relu(self.fc2(x))
    x=self.out(x)
    return x

torch.manual_seed(41)
model= Model()

criterion= nn.CrossEntropyLoss()
optimizer= torch.optim.Adam(model.parameters(), lr=0.01)
model.parameters

epochs = 100
losses = []

for i in range(epochs):
    y_pred = model(X_train)                      # forward pass
    loss = criterion(y_pred, y_train)            # compute loss
    loss_value = loss.item()                      # scalar loss
    losses.append(loss_value)

    if i % 10 == 0:
        print(f'Epoch: {i} Loss: {loss_value}')  # print every 10 epochs

    optimizer.zero_grad()  # clear gradients
    loss.backward()        # backpropagation
    optimizer.step()       # update weights

import torch
print('NaNs in X_train:', torch.isnan(X_train).sum().item())
print('NaNs in y_train:', torch.isnan(y_train).sum().item())
print('Infs in X_train:', torch.isinf(X_train).sum().item())
print('Infs in y_train:', torch.isinf(y_train).sum().item())

plt.plot(range(epochs), losses)  # losses already contain floats
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.title('Training Loss over Epochs')
plt.show()

with torch.no_grad():
    y_eval = model(X_test)
    loss = criterion(y_eval, y_test)
    print(f'Test Loss: {loss.item()}')

loss

with torch.no_grad():
    total_error = 0.0
    for i, data in enumerate(X_test):
        data = data.unsqueeze(0)  # Add batch dimension [1, features]
        y_val = model(data).item()
        true_val = y_test[i].item()

        print(f'{i+1:2}. Predicted: {y_val:6.2f}    True: {true_val:6.2f}')

        total_error += (y_val - true_val) ** 2  # squared error

    mse = total_error / len(y_test)
    print(f'\nMean Squared Error on test set: {mse:.4f}')

new_data= torch.tensor([16,26.1,3.0])
with torch.no_grad():
  pred= model.forward(new_data)
  print(pred.argmax().item())